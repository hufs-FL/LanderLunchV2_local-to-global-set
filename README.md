# Hybrid Federated-Distributed Reinforcement Learning Framework for LunarLander

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-ee4c2c.svg)](https://pytorch.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.68+-009688.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

A novel hybrid federated-distributed reinforcement learning framework that combines federated learning's global model integration with distributed reinforcement learning's parallel exploration capabilities for the LunarLander-v2 environment.

## ğŸ¯ Abstract

This framework addresses the limitations of traditional federated reinforcement learning by implementing a bidirectional learning architecture where both local clients and the global server perform independent learning. Unlike conventional approaches that rely on simple parameter averaging, our system leverages experience sharing and trust-aware aggregation to achieve faster convergence and better generalization.

## âœ¨ Key Features

- **Dual Learner Architecture**: Simultaneous learning at both local and global levels
- **Experience Data Aggregation**: Direct sharing of transition data for global learning
- **Trust-aware Parameter Aggregation**: Cosine similarity-based weighted aggregation
- **Transition Diversity Filtering**: Euclidean distance-based filtering (threshold: 0.15)
- **Asynchronous Update Scheduling**: Independent update cycles for experience learning (0.5s) and parameter aggregation (30s)
- **Multi-threaded Concurrency Control**: Hierarchical lock system for data consistency
- **Efficient Compression**: PyTorch state_dict serialization with gzip compression (~50% reduction)

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Global Server (FastAPI)                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          Dual Learning Components                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Experience-based â”‚    â”‚ Parameter-based    â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Learning (0.5s)  â”‚    â”‚ Aggregation (30s)  â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚           â”‚                         â”‚               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚     Global Experience Replay Buffer          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   (with Transition Diversity Filtering)      â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  API Endpoints:                                              â”‚
â”‚  â€¢ /upload-transition   (Experience data upload)             â”‚
â”‚  â€¢ /upload-weights     (Parameter upload with timestamp)     â”‚
â”‚  â€¢ /download-params    (Global model distribution)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP/REST
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client 1   â”‚   â”‚   Client 2   â”‚   â”‚  Client N    â”‚
â”‚              â”‚   â”‚              â”‚   â”‚              â”‚
â”‚ Local DQN/   â”‚   â”‚ Local DQN/   â”‚   â”‚ Local DQN/   â”‚
â”‚ Double DQN   â”‚   â”‚ Double DQN   â”‚   â”‚ Double DQN   â”‚
â”‚              â”‚   â”‚              â”‚   â”‚              â”‚
â”‚ LunarLander  â”‚   â”‚ LunarLander  â”‚   â”‚ LunarLander  â”‚
â”‚ Environment  â”‚   â”‚ Environment  â”‚   â”‚ Environment  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Performance Results

### 3-Client Configuration
| Method                                              | reward           | episode           | step                  | flops/episode     | total_flops      | reward/flops     |
|-----------------------------------------------------|------------------|-------------------|------------------------|-------------------|------------------|------------------|
| Basic Local DQN                                     | 190.181          | 528               | 1.98e+05               | 1.93e+09          | 1.04e+12         | 1.83e-10         |
| Basic Local Double DQN                              | 191.926          | 498               | 1.83e+05               | 1.78e+09          | 8.86e+11         | 2.17e+10         |
| 3Fed DQN (CCN-FedAvg)                               | 126.86 Â± 7.37    | 684.33 Â± 12.62     | 3.38e+05 Â± 6.58e+03     | 0.415             | 0.634            | 0.527            |
| 3Fed Double (CCN-FedAvg)                            | 77.51 Â± 0.39     | 1.33e+03 Â± 184.87  | 6.86e+05 Â± 6.65e+04     | 0.379             | 0.367            | 0.610            |
| **3Fusion DQN (Fed trust & buffer filter)**         | **217.93 Â± 3.21**| **406.67 Â± 3.50**  | **1.74e+05 Â± 7.42e+03** | **0.575**         | **0.541**        | **0.459**        |
| **3Fusion Double DQN (Fed trust & buffer filter)**  | **211.87 Â± 2.02**| **390.33 Â± 9.50**  | **1.82e+05 Â± 8.57e+03** | **0.468**         | **0.473**        | **0.556**        |

### 5-Client Configuration

| Method                                         | reward           | episode          | step               | flops/episode     | total_flops       | reward/flops      |
|-----------------------------------------------|------------------|------------------|--------------------|-------------------|-------------------|-------------------|
| Basic Local DQN                               | 190.181          | 528              | 1.98e+05           | 1.93e+09          | 1.04e+12          | 1.83e-10          |
| Basic Local Double DQN                        | 191.926          | 498              | 1.83e+05           | 1.78e+09          | 8.86e+11          | 2.17e+10          |
| 5Fed DQN (CCN-FedAvg)                         | 151.19 Â± 4.29    | 715.0 Â± 76.102   | 343289.0 Â± 24509.9 | 0.568 Â± 0.488     | 0.59 Â± 0.51       | 0.392 Â± 0.451     |
| 5Fed Double (CCN-FedAvg)                      | 151.92 Â± 22.4    | 629.8 Â± 103.35   | 281740.6 Â± 66470.8 | 0.60 Â± 0.408      | 0.56 Â± 0.423      | 0.291 Â± 0.41      |
| **5Fusion DQN** *(Fed trust&buffer filter)*   | **215.15 Â± 1.38**| **402.2 Â± 3.633**| **180887.2 Â± 5232.5**| **0.372 Â± 0.374**| **0.477 Â± 0.357** | **0.489 Â± 0.369** |
| **5Fusion Double DQN** *(Fed trust&buffer filter)* | **214.40 Â± 1.62**| **419.4 Â± 12.116**| **182899.0 Â± 4413.7**| **0.498 Â± 0.416**| **0.584 Â± 0.456** | **0.468 Â± 0.435** |

### Performance comparison depending on the use of buffer filtering
| Method                                              | reward           | episode           | step                  | flops/episode     | total_flops       | reward/flops      |
|-----------------------------------------------------|------------------|-------------------|------------------------|-------------------|-------------------|-------------------|
| 3Fusion Double DQN (Fed trust & buffer filter (x))  | 211.86 Â± 2.01    | 390.333 Â± 9.504   | 182201.33 Â± 8566.5     | 0.5               | 0.473 Â± 0.502     | 0.555 Â± 0.509     |
| 3Fusion Double DQN (Fed trust & buffer filter (o))  | 214.82 Â± 2.54    | 415.0 Â± 15.395    | 189389.6 Â± 10403.5     | 0.452 Â± 0.507     | 0.494 Â± 0.5       | 0.515 Â± 0.501     |
| 5Fusion Double DQN (Fed trust & buffer filter (x))  | 219.96 Â± 0.54    | 372.8 Â± 5.02      | 170791 Â± 9145.61       | 0.341 Â± 0.389     | 0.383 Â± 0.373     | 0.59 Â± 0.366      |
| 5Fusion Double DQN (Fed trust & buffer filter (o))  | 214.40 Â± 1.62    | 419.4 Â± 12.116    | 182899.0 Â± 4413.773    | 0.498 Â± 0.416     | 0.584 Â± 0.456     | 0.468 Â± 0.435     |



## ğŸ”§ Implementation Details

### Global Server Components

1. **Concurrency Control**
   - `buffer_lock`: Controls access to global replay buffer
   - `weight_lock`: Ensures parameter storage integrity
   - `aggregation_lock`: Maintains atomicity of parameter aggregation
   - Adaptive timeout mechanism to prevent deadlocks

2. **Communication Protocol**
   - RESTful API using FastAPI framework
   - Base64 encoding for JSON compatibility
   - Gzip compression for network efficiency

3. **Trust-aware Aggregation Formula**
   ```
   sim_k = cosine_similarity(Î¸_global, Î¸_k)
   w_k = max(0, sim_k) / Î£ max(0, sim_i)
   Î¸_global_new = Î±Â·Î¸_global + (1-Î±)Â·Î£(w_kÂ·Î¸_k)
   ```

### Transition Diversity Filtering

```
For new transition T = (s, a, r, s', d):
1. Sample k states {sâ‚, sâ‚‚, ..., sâ‚–} from buffer
2. Calculate D_t = (1/k)Â·Î£||s - sáµ¢||â‚‚
3. Accept T only if D_t â‰¥ Î¸ (Î¸ = 0.15)
```

## ğŸ“‹ Experimental Setup

| Parameter | Value |
|-----------|-------|
| Total Episodes | 2000 |
| Max Steps per Episode | 1000 |
| Convergence Threshold | 200 (average over 100 episodes) |
| Local Parameter Upload | Every 15 seconds |
| Global Aggregation | Every 40 seconds |
| Experience Buffer Upload | Every 30 seconds |
| Buffer Structure | deque |
| Learning Rate | 0.001 |
| Batch Size | 32 |

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9
- PyTorch 
- FastAPI
- OpenAI Gym (with LunarLander-v2)
- NumPy
- gzip, pickle, base64

### Installation

1. Clone the repository:
```bash
git clone https://github.com/hufs-FL/LanderLunchV2_global.git
cd LanderLunchV2_global
```

2. Install dependencies:
```bash
pip install --no-cache-dir \
    numpy \
    matplotlib \
    pandas \
    requests \
    fastapi \
    uvicorn \
    pydantic \
    cloudpickle \
    pyglet \
    scipy \
    pillow \
    tensorboard
```

### Running the System(Training Mode)
1. **Start the Global Server:**
```bash
# For Basic DQN
python global.py --agent_mode SIMPLE --network_mode SIMPLE --batch_size 64 --update_interval 0.5 --buffer_size 1000000 --port 5050

# For Double DQN
python global.py --test_mode=TRAIN --agent_mode DOUBLE --network_mode SIMPLE --batch_size 64 --update_interval 0.5 --buffer_size 1000000 --port 5050

# For Dueling DQN
python global.py --test_mode=TRAIN --agent_mode SIMPLE --network_mode DUELING --batch_size 64 --update_interval 0.5 --buffer_size 1000000 --port 5050

# For Double Dueling DQN
python global.py --test_mode=TRAIN --agent_mode DOUBLE --network_mode DUELING --batch_size 64 --update_interval 0.5 --buffer_size 1000000 --port 5050
```

## ğŸ”¬ Research Contributions

1. **Hybrid Architecture**: First to combine experience sharing with parameter aggregation in federated RL
2. **Dual Learning**: Simultaneous learning at both local and global levels
3. **Trust-aware Aggregation**: Novel weighting scheme based on policy similarity
4. **Diversity Filtering**: Maintains experience variety in global buffer

## ğŸ“š Citation

If you use this code in your research, please cite:


@article{jeon2025hybrid,
  title={A Hybrid Federated-Distributed Reinforcement Learning Framework 
         for Efficient Policy Generalization and Global Convergence},
  author={Jeon, Byunghwan},
  journal={Hankuk University of Foreign Studies},
  year={2025}
}

## ğŸ­ Applications

- **Smart Factories**: Real-time adaptive control with rapid user requirement adaptation
- **Autonomous Driving**: Distributed learning from multiple vehicles
- **Large-scale IoT**: Sensor-based systems with non-sensitive data
- **Robotics**: Multi-robot coordination and policy learning

## âš ï¸ Limitations and Considerations

1. **Security Trade-off**: Unlike traditional FL, this framework shares transition data, which may have privacy implications
2. **Network Overhead**: Continuous experience sharing requires stable network connectivity
3. **Scalability**: Tested with up to 5 clients; larger scales require additional optimization

## ğŸ”® Future Work

-  Extension to high-dimensional continuous control environments
-  Integration with policy gradient methods (PPO, SAC)
-  Communication efficiency optimization
-  Enhanced security features for sensitive applications
-  Distributed global server architecture

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <i>Advancing the frontier of federated reinforcement learning through efficient hybrid architectures</i>
</p>
